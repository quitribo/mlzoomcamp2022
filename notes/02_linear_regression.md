> Previous: [Intro to Machine Learning](01_intro.md)

> [Back to Index](README.md)

> Next: [Classification / Logistic Regression](03_classification.md)

# Linear regression

## General formula for ML

* `g(X) â‰ˆ y`
* `g` = model
* `X` = feature matrix
* `y` = target

For our Linear Regression example, `g` will be Linear Regression model, our `y` target will be Price.

## Simplified model (one single observation)

* `g(Xáµ¢) â‰ˆ yáµ¢`
* `Xáµ¢` = a car
* `yáµ¢` = price of the car
* `Xáµ¢ = (xáµ¢â‚, xáµ¢â‚‚, ..., xáµ¢â‚™)`
* Each `xáµ¢â±¼` is a characteristic (feature) of the car.

Thus

* `g(xáµ¢â‚, xáµ¢â‚‚, ..., xáµ¢â‚™) â‰ˆ yáµ¢`

For our example, we'll pick 3 features: horse power, milles per gallon and popularity.

* `Xáµ¢ = [453, 11, 86]`

Here is the formula for our Linear Regression.

* `g(Xáµ¢) = wâ‚€ + wâ‚Â·xáµ¢â‚ + wâ‚‚Â·xáµ¢â‚‚ + wâ‚ƒÂ·xáµ¢â‚ƒ`
* `wâ‚€` is the ***bias term*** weight.
* All other `wâ±¼` are weights for each of the features.

Alternatively:

* `g(Xáµ¢) = wâ‚€ + âˆ‘( wâ±¼Â·xáµ¢â±¼, j=[1,3] )`

Depending on the values of the weights, our predicted price `yáµ¢` will be different.

## Linear Regression in vector form

Linear Regression formula for `n` features:

*  `g(Xáµ¢) = wâ‚€ + âˆ‘( wâ±¼Â·xáµ¢â±¼, j=[1,n] )`

The `âˆ‘( wâ±¼Â·xáµ¢â±¼, j=[1,n] )` term is actually a ***dot product***. Thus:

* `g(Xáµ¢) = wâ‚€ + Xáµ¢áµ€ Â· W`
* `Xáµ¢áµ€` is the transposed feature vector.
* `W` is the weights vector.

We can make this formula even shorter by incorporate the bias term `w0` to our dot product, "simulating" a new feature `xi0` which is always equal to one.

* `W = [wâ‚€, wâ‚, wâ‚‚, ... , wâ‚™]`
* `Xáµ¢ = [1, xáµ¢â‚, xáµ¢â‚‚, ... , xáµ¢â‚™]`
* `Wáµ€ Â· Xáµ¢ = Xáµ¢áµ€ Â· W`

We have now converted the complete Linear Regression formula into a dot product of vectors.

Generalizing for multiple observations, `X` now becomes a matrix of size `m * n` where `n` is the number of features just like before, and `m` is the number of observations. Each row of `X` is an observation, identical in form to our previous `Xáµ¢`.

        1   xâ‚â‚   xâ‚â‚‚   ...   xâ‚â‚™
        1   xâ‚‚â‚   xâ‚‚â‚‚   ...   xâ‚‚â‚™
        ... ...   ...   ...   ...
        1   xâ‚˜â‚   xâ‚˜â‚‚   ...   xâ‚˜â‚™ 

We can now multiply `X` with `W` to get our predictions.

        1   xâ‚â‚   xâ‚â‚‚   ...   xâ‚â‚™       wâ‚€        Xâ‚áµ€Â·W
        1   xâ‚‚â‚   xâ‚‚â‚‚   ...   xâ‚‚â‚™   Â·   wâ‚    =   Xâ‚‚áµ€Â·W 
        ... ...   ...   ...   ...       ...       ...
        1   xâ‚˜â‚   xâ‚˜â‚‚   ...   xâ‚˜â‚™       wâ‚™        Xâ‚˜áµ€Â·W

The resulting vector `Å¶` is the prediction vector.

But how do we calculate the weights?

## Normal equation

* `g(X) = XÂ·W â‰ˆ y`

We want to solve for `W`. We invert `X` and use it to solve the equation:

* `Xâ»Â¹ Â· X Â· W = Xâ»Â¹ Â· y`
* `Xâ»Â¹ Â· X = I`

Thus:
* `I Â· W = Xâ»Â¹ Â· y` -> `I` is the Identity Matrix and does not change `W`.
* `W = Xâ»Â¹ Â· y`

However, `X` may not be a square matrix and there is no guarantee that `Xâ»Â¹` will exist at all. However, there is a workaround using transposed matrices:

* `Xáµ€ Â· X` -> Gram matrix. A Gram matrix is ALWAYS square because it's of size `(n+1) * (n+1)`. Thus, it can be inverted.

We can now calculate the inverse of the Gram matrix and use it to solve our equation:

* Starting from `Xáµ€ Â· X Â· W = Xáµ€ Â· y`, we use the inverse of the Gram matrix to cancel out the terms.
* `(Xáµ€ Â· X)â»Â¹ Â· Xáµ€ Â· X Â· W = (Xáµ€ Â· X)â»Â¹ Â· Xáµ€Â· y`

Thus, knowing that `(Xáµ€ Â· X)â»Â¹ Â· Xáµ€  Â· X = I` and can therefore be cancelled out, we finally get the closest solution possible for `W`:

* `W = (Xáµ€  Â· X)â»Â¹ Â· Xáµ€  Â· y`

## RMSE (Root Mean Square Error)

MSRE is a convenient way to measure the accuracy (or the error) of our model.

* `RMSE = âˆš( 1/m * âˆ‘( (g(Xáµ¢) - yáµ¢)Â²	 , i=[1,m] ) )`
* `g(Xáµ¢)` is the prediction for `Xáµ¢`.
* `yáµ¢` is the actual value.

The lower the RMSE, the more accurate the model is.

## Categorical features

Some features are categorical rather than numerical in nature. For example, `Car_maker` would be a categorical feature.

We can encode these features as ***one-hot vectors***.  A one-hot vector is a vector with as many elements as elements there are in a category, and all of the elements are `0` except for a single `1` representing a particular element.

* `category = ['cat', 'dog', 'bird']`
* `cat = [1,0,0]`
* `dog = [0,1,0]`
* `bird = [0,0,1]`

In pandas, we simply add a new feature to the DataFrame for each category element with value `0` or `1`. For the example above, we would add 3 columns to the DataFrame.

## Regularization

âœ… Regularization is a technique that penalizes the complexity of models, in other words, allow ensuring the validity of our model by avoiding overfitting the training data. 

Sometimes there are features which are _linear combinations_ of other features (sum/product of other columns). This can lead to columns in the `X` matrix which are identical. The consequence of this is that the Gram matrix `Xáµ€ Â· X` becomes a _singular matrix_ and thus cannot be inverted.

ðŸ”° In regularization, we have a trade-off between overfitting and underfitting our data.

ðŸ‘‰ If we select a small value of regularization that tends to zero, it means no penalization to the model, leaving the complexity of the model in the initial state, and increasing the chances of overfitting. 

ðŸ‘‰ If we select a high value of regularization that penalizes the complexity of the model by shrinking the weights, which increases the chance to fit the model by removing all complexity.

In the case of noisy data which can lead to almost-but-not-quite identical features, the Gram matrix is invertable but the resulting values within are disproportionally big. This distorts the training and leads to huge errors.

We can solve this with ***regularization***. We will use a _regularization parameter_ that will modify our normal equation in a way that will result in greatly reduced weights.

For Linear Regression, regularization is applied to the diagonal of the Gram matrix. The regularization parameter is simply added to the diagonal.

The regularization parameter is usually a small decimal value such as `0.00001` or `0.01`. The larger the parameter, the smaller the final values in the inverted matrix will be. However, a very big regularization parameter can lead to worse performance than smaller parameters.

In numpy, we can implement regularization easily by creating an identity matrix with `np.eye()`, multiplying it with our regularization parameter and finally adding the resultant matrix to the Gram matrix.

## Tuning the model

We want to find the best value for the regularization parameter. For Linear Regression, we can simply try multiple parameters in a for loop during training because the computational cost is low.

## Linear Regression workflow recap

1. Explore the data
    1. Understand the target distribution.
    1. Find out which changes you need to do to it in order to use it.
1. Clean up the data.
    1. Do transformations on it such as getting rid of spaces, lower case everything, fill in the NaNs, etc.
1. Prepare the data.
    1. Apply feature engineering -> convert categorical features into numerical ones with one-hot encoding.
    1. Shuffle and split the data into train-validation-test splits.
    1. Create the feature matrix for each split; make sure that you add the "virtual bias" column to it.
1. Train the model.
    1. Use the normal equation to calculate the weights.
    1. Make sure you apply a regularization parameter.
1. Tune the model
    1. Use RMSE to check accuracy.
    1. Predict values with the validation dataset and compare with the ground truth.
    1. Adjust regularization parameter accordingly.
    1. Plot histogram for easy visual check.
1. Test your model with the test dataset.
1. Use your model!

> Previous: [Intro to Machine Learning](01_intro.md)

> [Back to Index](README.md)

> Next: [Classification / Logistic Regression](03_classification.md)
